{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING PACKAGES\n",
    "# document_inverted_index is the inverted index with word and its corresponding postings\n",
    "# document_Tweets_mapping is the mapping between document and tweet\n",
    "# Total_documents variable holds the total number of variales which is used in calculating the TF-IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import math \n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweets = open('tweets_corpus.txt')\n",
    "lines = tweets.readlines()\n",
    "document_inverted_index = {}\n",
    "document_tweets_mapping = {}\n",
    "Total_documents = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the punctuations, stop words, and convert all the words to lower case words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    #tokenized_words = nltk.word_tokenize(tweet)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    processed_words = [token.lower() for token in tokenizer.tokenize(tweet) if token.lower() not in stop_words]\n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize each word. Lemmatize them and convert them to their base word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet(processed_vocabs):\n",
    "    normalized_vocabs = []\n",
    "    porter = WordNetLemmatizer()\n",
    "    for vocab in processed_vocabs:\n",
    "        normalized_vocabs.append(porter.lemmatize(vocab))\n",
    "    return normalized_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read each line and create inverted index by parsing the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(line):\n",
    "    list_docids_strings = line.split('\\t')\n",
    "    document_id = list_docids_strings[0]\n",
    "    tweet = ''.join(list_docids_strings[1:]) \n",
    "    document_tweets_mapping[document_id] = tweet\n",
    "    processed_vocabs = clean_text(tweet)\n",
    "    normalized_vocabs = normalize_tweet(processed_vocabs)\n",
    "    \n",
    "    for each_vocab in normalized_vocabs:\n",
    "        if each_vocab in document_inverted_index:\n",
    "            vocab_postings = document_inverted_index[each_vocab]\n",
    "            if document_id in vocab_postings:\n",
    "                continue\n",
    "            else:\n",
    "                vocab_postings.append(document_id)\n",
    "                vocab_postings.sort()\n",
    "        else:\n",
    "            postings = []\n",
    "            postings.append(document_id)\n",
    "            document_inverted_index[each_vocab] = postings\n",
    "for line in lines:\n",
    "    Total_documents = Total_documents + 1\n",
    "    create_inverted_index(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVERTED INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bandaging': ['81499877556760576'],\n",
       " 'paper': ['81499877556760576'],\n",
       " 'cut': ['81499877556760576'],\n",
       " 'cheesecake': ['81499877556760576', '81600113016971264', '81716618236928000'],\n",
       " 'dinner': ['81499877556760576'],\n",
       " 'calling': ['81499877556760576'],\n",
       " 'night': ['81499877556760576', '82650970722533376'],\n",
       " 'doin': ['81499877556760576'],\n",
       " 'big': ['81499877556760576'],\n",
       " 'nyc': ['81499877556760576'],\n",
       " 'krispy': ['81500716438523904'],\n",
       " 'kremes': ['81500716438523904'],\n",
       " 'strawberry': ['81500716438523904',\n",
       "  '81623945064890368',\n",
       "  '81656304107651072',\n",
       "  '81715158593966080',\n",
       "  '81716618236928000',\n",
       "  '81842384404623360',\n",
       "  '81844590625304576',\n",
       "  '82461950231064576',\n",
       "  '85032815321825280',\n",
       "  '85094773555335168'],\n",
       " 'trifle': ['81500716438523904'],\n",
       " 'since': ['81500716438523904'],\n",
       " 'started': ['81500716438523904'],\n",
       " 'gym': ['81500716438523904'],\n",
       " 'cry': ['81500716438523904'],\n",
       " 'bacon': ['81503002321616896',\n",
       "  '81736742478155778',\n",
       "  '82650970722533376',\n",
       "  '85032815321825280',\n",
       "  '86441828815089664'],\n",
       " 'cheddar': ['81503002321616896'],\n",
       " 'slider': ['81503002321616896'],\n",
       " 'topped': ['81503002321616896'],\n",
       " 'w': ['81503002321616896', '81507775422791680', '86441828815089664'],\n",
       " 'fried': ['81503002321616896'],\n",
       " 'egg': ['81503002321616896',\n",
       "  '81587643376336896',\n",
       "  '81673244926685184',\n",
       "  '81736742478155778',\n",
       "  '82650970722533376',\n",
       "  '85032815321825280',\n",
       "  '86441828815089664'],\n",
       " 'blue': ['81503002321616896'],\n",
       " 'cheese': ['81503002321616896',\n",
       "  '81507775422791680',\n",
       "  '81534165975171072',\n",
       "  '81535634019323904',\n",
       "  '81577509950459904',\n",
       "  '81582996083326976',\n",
       "  '81587643376336896',\n",
       "  '81673244926685184',\n",
       "  '81736742478155778',\n",
       "  '82650970722533376',\n",
       "  '85032815321825280',\n",
       "  '85094773555335168',\n",
       "  '86441828815089664'],\n",
       " 'avocado': ['81503002321616896'],\n",
       " 'purple': ['81503002321616896'],\n",
       " 'cherokee': ['81503002321616896'],\n",
       " 'tomato': ['81503002321616896'],\n",
       " 'nacho': ['81507775422791680'],\n",
       " 'shirt': ['81507775422791680'],\n",
       " 'uggghhh': ['81507775422791680'],\n",
       " 'aint': ['81534165975171072'],\n",
       " 'nuffin': ['81534165975171072'],\n",
       " 'piece': ['81534165975171072'],\n",
       " 'without': ['81534165975171072'],\n",
       " 'corner': ['81534165975171072'],\n",
       " 'word': ['81534165975171072'],\n",
       " 'never': ['81534165975171072'],\n",
       " 'slice': ['81534165975171072'],\n",
       " 'bitch': ['81534165975171072'],\n",
       " 'tag_username': ['81535634019323904',\n",
       "  '81582996083326976',\n",
       "  '81587643376336896',\n",
       "  '81715158593966080',\n",
       "  '81716618236928000',\n",
       "  '81842384404623360',\n",
       "  '81844590625304576',\n",
       "  '86441828815089664'],\n",
       " 'mmmm': ['81535634019323904', '81577509950459904'],\n",
       " 'dreaming': ['81535634019323904'],\n",
       " 'squirrel': ['81535634019323904'],\n",
       " 'burger': ['81535634019323904'],\n",
       " '1st': ['81582996083326976'],\n",
       " 'like': ['81582996083326976'],\n",
       " '1': ['81582996083326976'],\n",
       " 'year': ['81582996083326976'],\n",
       " 'younger': ['81582996083326976'],\n",
       " 'u': ['81582996083326976'],\n",
       " '2nd': ['81582996083326976'],\n",
       " 'age': ['81582996083326976'],\n",
       " 'number': ['81582996083326976'],\n",
       " '3rd': ['81582996083326976'],\n",
       " 'ima': ['81582996083326976'],\n",
       " 'cater': ['81582996083326976'],\n",
       " 'ur': ['81582996083326976'],\n",
       " 'wedding': ['81582996083326976'],\n",
       " 'wit': ['81582996083326976'],\n",
       " 'patty': ['81582996083326976'],\n",
       " 'n': ['81582996083326976'],\n",
       " 'rt': ['81587643376336896', '86441828815089664'],\n",
       " 'want': ['81587643376336896',\n",
       "  '81673244926685184',\n",
       "  '81716618236928000',\n",
       "  '86441828815089664'],\n",
       " 'steak': ['81587643376336896'],\n",
       " 'roll': ['81587643376336896'],\n",
       " 'right': ['81587643376336896', '81673244926685184'],\n",
       " 'think': ['81600113016971264', '81673244926685184'],\n",
       " 'imma': ['81600113016971264'],\n",
       " 'eat': ['81600113016971264', '81644157432643584'],\n",
       " 'befor': ['81600113016971264'],\n",
       " 'lay': ['81600113016971264'],\n",
       " 'havent': ['81600113016971264'],\n",
       " 'mixed': ['81623945064890368'],\n",
       " 'one': ['81623945064890368', '81842384404623360'],\n",
       " 'mostly': ['81623945064890368'],\n",
       " 'peach': ['81623945064890368'],\n",
       " 'little': ['81623945064890368'],\n",
       " 'white': ['81623945064890368'],\n",
       " 'cherry': ['81623945064890368'],\n",
       " 'dr': ['81623945064890368'],\n",
       " 'pepper': ['81623945064890368'],\n",
       " 'coke': ['81623945064890368'],\n",
       " 'etc': ['81623945064890368'],\n",
       " 'stomach': ['81644157432643584'],\n",
       " 'yelling': ['81644157432643584'],\n",
       " 'telling': ['81644157432643584'],\n",
       " 'get': ['81644157432643584', '81716618236928000', '81842384404623360'],\n",
       " 'as': ['81644157432643584'],\n",
       " 'nd': ['81644157432643584'],\n",
       " 'somethin': ['81644157432643584'],\n",
       " 'went': ['81644157432643584', '82650970722533376'],\n",
       " 'got': ['81644157432643584'],\n",
       " 'cheesesticks': ['81644157432643584'],\n",
       " 'waterbottle': ['81644157432643584'],\n",
       " 'chocolate': ['81656304107651072', '85094773555335168'],\n",
       " 'mint': ['81656304107651072'],\n",
       " 'cooky': ['81656304107651072'],\n",
       " 'cream': ['81656304107651072', '85094773555335168'],\n",
       " 'berry': ['81656304107651072'],\n",
       " 'caramel': ['81656304107651072'],\n",
       " 'blend': ['81656304107651072'],\n",
       " 'perfectly': ['81656304107651072'],\n",
       " 'mouth': ['81656304107651072'],\n",
       " 'pancake': ['81673244926685184', '86441828815089664'],\n",
       " 'cook': ['81673244926685184'],\n",
       " 'gf': ['81673244926685184'],\n",
       " 'make': ['81673244926685184'],\n",
       " 'hard': ['81673244926685184'],\n",
       " 'decision': ['81673244926685184'],\n",
       " 'totally': ['81715158593966080'],\n",
       " 'also': ['81715158593966080'],\n",
       " 'good': ['81715158593966080', '82650970722533376', '86441828815089664'],\n",
       " 'fish': ['81715158593966080'],\n",
       " 'chicken': ['81715158593966080'],\n",
       " 'veggie': ['81715158593966080', '85032815321825280'],\n",
       " 'oh': ['81715158593966080'],\n",
       " 'dessert': ['81715158593966080'],\n",
       " 'drizzling': ['81715158593966080'],\n",
       " 'tart': ['81715158593966080'],\n",
       " 'party': ['81715158593966080'],\n",
       " 'tonight': ['81715158593966080'],\n",
       " 'c': ['81715158593966080'],\n",
       " 'candy': ['81716618236928000'],\n",
       " 'bear': ['81716618236928000'],\n",
       " '2': ['81716618236928000'],\n",
       " '38': ['81716618236928000'],\n",
       " 'expire': ['81716618236928000'],\n",
       " 'made': ['81736742478155778'],\n",
       " 'scrambled': ['81736742478155778', '85032815321825280'],\n",
       " 'bit': ['81736742478155778'],\n",
       " 'could': ['81842384404623360'],\n",
       " 'try': ['81842384404623360'],\n",
       " 'great': ['81842384404623360'],\n",
       " 'haha': ['81842384404623360'],\n",
       " 'recommend': ['81842384404623360'],\n",
       " 'thats': ['81842384404623360'],\n",
       " 'fave': ['81842384404623360'],\n",
       " 'im': ['81844590625304576'],\n",
       " '100': ['81844590625304576'],\n",
       " 'pure': ['81844590625304576'],\n",
       " 'flavoured': ['81844590625304576'],\n",
       " 'hmmm': ['81844590625304576'],\n",
       " 'tasty': ['81844590625304576'],\n",
       " 'beautiful': ['82461950231064576'],\n",
       " 'pink': ['82461950231064576'],\n",
       " 'coloured': ['82461950231064576'],\n",
       " 'raspberry': ['82461950231064576'],\n",
       " 'banana': ['82461950231064576'],\n",
       " 'smoothie': ['82461950231064576'],\n",
       " 'swimming': ['82650970722533376'],\n",
       " 'ate': ['82650970722533376'],\n",
       " 'asparagus': ['82650970722533376'],\n",
       " 'biscuit': ['82650970722533376'],\n",
       " 'goodness': ['82650970722533376'],\n",
       " 'watched': ['82650970722533376'],\n",
       " 'date': ['82650970722533376'],\n",
       " 'tag_final_hashtags': ['82650970722533376', '85032815321825280'],\n",
       " 'hashbrowns': ['85032815321825280'],\n",
       " 'turkey': ['85032815321825280'],\n",
       " 'tofu': ['85032815321825280'],\n",
       " 'scramble': ['85032815321825280'],\n",
       " 'rice': ['85032815321825280'],\n",
       " 'french': ['85032815321825280'],\n",
       " 'toast': ['85032815321825280'],\n",
       " 'cantaloupe': ['85032815321825280'],\n",
       " 'tag_hashtags': ['85094773555335168', '86441828815089664'],\n",
       " 'using': ['85094773555335168'],\n",
       " 'icing': ['85094773555335168'],\n",
       " 'cake': ['85094773555335168'],\n",
       " 'use': ['85094773555335168'],\n",
       " 'vanilla': ['85094773555335168'],\n",
       " 'hashbrown': ['86441828815089664'],\n",
       " 'casserole': ['86441828815089664'],\n",
       " 'deck': ['86441828815089664'],\n",
       " 'sum': ['86441828815089664'],\n",
       " 'ok': ['86441828815089664']}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersect (AND) two posting lists  and merge them using the least length posting first algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersect_AND_two_postings(posting1, posting2):\n",
    "    Intersection = []\n",
    "    least = []\n",
    "    major = []\n",
    "    if (len(posting1) > len(posting2)):\n",
    "        least = posting2.copy()\n",
    "        major = posting1.copy()\n",
    "    else:\n",
    "        least = posting1.copy()\n",
    "        major = posting2.copy()\n",
    "    \n",
    "    for least_posting_element in least:\n",
    "        if (least_posting_element in major):\n",
    "            Intersection.append(least_posting_element)\n",
    "        else:\n",
    "            continue\n",
    "    return Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNION (OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersect_OR_two_postings(posting1, posting2):\n",
    "    return list(set(posting1 + posting2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A NOT B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersect_A_NOT_B_two_postings(posting1, posting2):\n",
    "    return list(set(posting1) - set(posting2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function To pring the intersecting documents and the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matched_documents(intersection):\n",
    "    for document in intersection:\n",
    "        print('DOCUMENT:', document)\n",
    "        print(document_tweets_mapping[document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"good and fish\" query is processed and the documents retrieved after processing them\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 81715158593966080\n",
      "TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = 'good'\n",
    "query2 = 'fish'\n",
    "if ((query1 in document_inverted_index) and (query2 in document_inverted_index)):\n",
    "    posting1 = document_inverted_index[query1]\n",
    "    posting2 = document_inverted_index[query2]\n",
    "    interesction= Intersect_AND_two_postings(posting1, posting2)\n",
    "    print_matched_documents(interesction)\n",
    "else:\n",
    "    print('GIVEN QUERY DOES NOT EXIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"bacon or fish\" query is processed and the documents retreived after processing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 81736742478155778\n",
      "Made myself some scrambled eggs with cheese and bacon bits\n",
      "\n",
      "DOCUMENT: 81503002321616896\n",
      "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\n",
      "\n",
      "DOCUMENT: 82650970722533376\n",
      "I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 81715158593966080\n",
      "TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\n",
      "\n",
      "DOCUMENT: 86441828815089664\n",
      "RT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = 'bacon'\n",
    "query2 = 'fish'\n",
    "posting1 = document_inverted_index[query1]\n",
    "posting2 = document_inverted_index[query2]\n",
    "interesction= Intersect_OR_two_postings(posting1, posting2)\n",
    "print_matched_documents(interesction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"bacon and not fish\" query is processed and documents are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 81736742478155778\n",
      "Made myself some scrambled eggs with cheese and bacon bits\n",
      "\n",
      "DOCUMENT: 81503002321616896\n",
      "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\n",
      "\n",
      "DOCUMENT: 82650970722533376\n",
      "I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 86441828815089664\n",
      "RT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = 'bacon'\n",
    "query2 = 'fish'\n",
    "posting1 = document_inverted_index[query1]\n",
    "posting2 = document_inverted_index[query2]\n",
    "interesction= Intersect_A_NOT_B_two_postings(posting1, posting2)\n",
    "print_matched_documents(interesction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for AND multiple queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_algorithm_intersection(query_list):\n",
    "    temp_dict = {}\n",
    "    for query in query_list:\n",
    "        temp_dict[query] = len(document_inverted_index[query])\n",
    "    list_of_items = sorted(temp_dict.items(), key= lambda k:k[1])\n",
    "    sorted_query = []\n",
    "    for item in list_of_items:\n",
    "        sorted_query.append(item[0])\n",
    "    \n",
    "    Result = []\n",
    "    if len(sorted_query) == 0:\n",
    "        return Result\n",
    "    \n",
    "    Result = document_inverted_index[sorted_query[0]]\n",
    "    if len(sorted_query) == 1:\n",
    "        return Result\n",
    "    \n",
    "    for index in range(1, len(sorted_query)):\n",
    "        query = sorted_query[index]\n",
    "        posting = document_inverted_index[query]\n",
    "        Result = Intersect_AND_two_postings(Result, posting)\n",
    "    print_matched_documents(Result)\n",
    "    return Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"scrambled and cheese and bacon and turkey\" and results are retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['85032815321825280']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'scrambled and cheese and bacon and turkey'\n",
    "Merge_algorithm_intersection(str1.split(' and '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"pure and strawberry and flovoured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = 'pure and strawberry and flavoured'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 81844590625304576\n",
      "TAG_USERNAME then im 100% pure strawberry flavoured hmmm tasty\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['81844590625304576']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merge_algorithm_intersection(str2.split(' and '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"cheese and hashbrowns and toast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['85032815321825280']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str3 = 'cheese and hashbrowns and toast'\n",
    "Merge_algorithm_intersection(str3.split(' and '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Or_And_algorithm(query):\n",
    "    or_queries = query.split(' or ')\n",
    "    Result = []\n",
    "    for each_query in or_queries:\n",
    "        print(each_query)\n",
    "        if 'and' in each_query:\n",
    "            if len(Result)==0 or Result==None:\n",
    "                Result = Merge_algorithm_intersection(each_query.split(' and '))\n",
    "            else:\n",
    "                posting1 = Result\n",
    "                posting2 = Merge_algorithm_intersection(each_query.split(' and '))\n",
    "                Result = Intersect_AND_two_postings(posting1, posting2)\n",
    "            continue\n",
    "        else:\n",
    "            posting1 = Result\n",
    "            posting2 = document_inverted_index[each_query]\n",
    "            Result = Intersect_OR_two_postings(posting1, posting2)\n",
    "            \n",
    "    return Result   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"cheese and hashbrowns and toast or biscuit or dessert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheese and hashbrowns and toast\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "biscuit\n",
      "dessert\n",
      "DOCUMENT: 82650970722533376\n",
      "I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "DOCUMENT: 81715158593966080\n",
      "TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str4 = 'cheese and hashbrowns and toast or biscuit or dessert'\n",
    "print_matched_documents(Merge_Or_And_algorithm(str4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'fish and good and chicken or cheese and hashbrowns or toast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish and good and chicken\n",
      "DOCUMENT: 81715158593966080\n",
      "TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\n",
      "\n",
      "cheese and hashbrowns\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "toast\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str5 = 'fish and good and chicken or cheese and hashbrowns or toast'\n",
    "print_matched_documents(Merge_Or_And_algorithm(str5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to calculate the TF IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences(word, sentence):\n",
    "    return sentence.lower().split().count(word)\n",
    "\n",
    "def calculate_tf_idf_scrore(query, document):\n",
    "    sentence = document_tweets_mapping[document]\n",
    "    total_terms = len(sentence.split())\n",
    "    \n",
    "    total_tf_idf = 0\n",
    "    \n",
    "    or_quries = query.split(' or ')\n",
    "    for each_or_querie in or_quries:\n",
    "        if 'and' in each_or_querie:\n",
    "            and_queries = each_or_querie.split(' and ')\n",
    "            for each_and_querie in and_queries:\n",
    "                occurences_of_word_indocument = count_occurrences(each_and_querie, sentence)\n",
    "                tf_score_word = (occurences_of_word_indocument/total_terms)\n",
    "                idf_score = math.log(Total_documents/len(document_inverted_index[each_and_querie]))\n",
    "                tf_idf_score_perword = tf_score_word * idf_score\n",
    "                total_tf_idf = total_tf_idf + tf_idf_score_perword\n",
    "        else:\n",
    "            occurences_of_word_indocument = count_occurrences(each_or_querie, sentence)\n",
    "            tf_score_word = (occurences_of_word_indocument/total_terms)\n",
    "            idf_score = math.log(Total_documents/len(document_inverted_index[each_or_querie]))\n",
    "            tf_idf_score_perword = tf_score_word * idf_score\n",
    "            total_tf_idf = total_tf_idf + tf_idf_score_perword\n",
    "            \n",
    "    return total_tf_idf\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process \"fish and good and chicken or cheese and hashbrowns or toast\" and print the document and print the TF IDF score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish and good and chicken\n",
      "DOCUMENT: 81715158593966080\n",
      "TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\n",
      "\n",
      "cheese and hashbrowns\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "toast\n",
      "fish and good and chicken or cheese and hashbrowns or toast\n",
      "TF-IDF SCORE 0.3030092231992304\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str6 = 'fish and good and chicken or cheese and hashbrowns or toast'\n",
    "Matched_Documents = Merge_Or_And_algorithm(str6)\n",
    "for document in Matched_Documents:\n",
    "    print(str6)\n",
    "    print('TF-IDF SCORE', calculate_tf_idf_scrore(str6, document))\n",
    "    print(document_tweets_mapping[document])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process \"cheese and hashbrowns\" and print the document and print the TF IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheese and hashbrowns\n",
      "DOCUMENT: 85032815321825280\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n",
      "cheese and hashbrowns\n",
      "TF-IDF SCORE 0.3030092231992304\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str7 = 'cheese and hashbrowns'\n",
    "Matched_Documents = Merge_Or_And_algorithm(str7)\n",
    "for document in Matched_Documents:\n",
    "    print(str7)\n",
    "    print('TF-IDF SCORE', calculate_tf_idf_scrore(str6, document))\n",
    "    print(document_tweets_mapping[document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
